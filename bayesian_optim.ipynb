{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ParameterSampler, RandomizedSearchCV, cross_val_score\n",
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import pickle\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, features_fore_linear=25088, dataset=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Helper hyperparameters to keep track of VGG16 architecture\n",
    "        pool_stride = 2\n",
    "        conv_kernel = 3\n",
    "        pool_kernel = 2\n",
    "        dropout_probs = 0.5\n",
    "        optim_momentum = 0.9\n",
    "        weight_decay = 5e-4\n",
    "        learning_rate = 1e-4\n",
    "\n",
    "        # Define features and classifier each individually, this is how the VGG16-D model is orignally defined\n",
    "        self.features = torch.nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=conv_kernel, padding=1), \n",
    "            nn.BatchNorm2d(64),  # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=conv_kernel, padding=1), \n",
    "            nn.BatchNorm2d(64),  # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=conv_kernel),\n",
    "            nn.BatchNorm2d(128),  # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=conv_kernel),\n",
    "            nn.BatchNorm2d(128),  # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "            \n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=conv_kernel),\n",
    "            nn.BatchNorm2d(256),  # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=conv_kernel),\n",
    "            nn.BatchNorm2d(256),  # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=conv_kernel),\n",
    "            nn.BatchNorm2d(256),  # Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride),\n",
    "        ).to(device)\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=features_fore_linear, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_probs),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_probs),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes),\n",
    "        ).to(device)\n",
    "        \n",
    "        # In the paper, they mention updating towards the 'multinomial logistic regression objective'\n",
    "        # As can be read in Bishop p. 159, taking the logarithm of this equates to the cross-entropy loss function.\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer - For now just set to Adam to test the implementation\n",
    "        self.optim = torch.optim.Adam(list(self.features.parameters()) + list(self.classifier.parameters()), lr=learning_rate)\n",
    "        #self.optim = torch.optim.SGD(list(self.features.parameters()) + list(self.classifier.parameters()), lr=learning_rate, momentum=optim_momentum, weight_decay=weight_decay)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "    def train_model(self, train_dataloader, epochs=1, val_dataloader=None):\n",
    "        \n",
    "        # Call .train() on self to turn on dropout\n",
    "        self.train()\n",
    "\n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "        \n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # Keep track of training accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "\n",
    "            # If val_dataloader, evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                # Turn off dropout for testing\n",
    "                self.eval()\n",
    "                acc = self.eval_model(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}, test accuracy: {epoch_acc / len(train_dataloader.dataset)}\")\n",
    "                # turn on dropout after being done\n",
    "                self.train()\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval_model(self, test_dataloader):\n",
    "        \n",
    "        self.eval()\n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            total_acc += (torch.argmax(logits, dim=1) == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "\n",
    "    def predict(self, img_path):\n",
    "        img = PIL.Image.open(img_path)\n",
    "        img = self.dataset.dataset.transform(img)\n",
    "        classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "        return img, classification\n",
    "    \n",
    "    def predict_random(self, num_predictions=16):\n",
    "        \"\"\"\n",
    "        Plot random images from own given dataset\n",
    "        \"\"\"\n",
    "        random_indices = np.random.choice(len(self.dataset)-1, num_predictions, replace=False)\n",
    "        classifcations = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        for idx in random_indices:\n",
    "            img, label = self.dataset.__getitem__(idx)\n",
    "            device = next(self.parameters()).device  # Get model's device\n",
    "            img = img.to(device).unsqueeze(0)  # Ensure correct shape: (1, C, H, W)\n",
    "            # Move image to same device\n",
    "            \n",
    "            classifcation = torch.argmax(self(img), dim=1)\n",
    "\n",
    "            classifcations.append(classifcation)\n",
    "            labels.append(label)\n",
    "            images.append(img)\n",
    "\n",
    "        return classifcations, labels, images\n",
    "\n",
    "def get_vgg_weights(model):\n",
    "    \"\"\"\n",
    "    Loads VGG16-D weights for the classifier to an already existing model\n",
    "    Also sets training to only the classifier\n",
    "    \"\"\"\n",
    "    # Load the complete VGG16 model\n",
    "    temp = torchvision.models.vgg16(weights='DEFAULT')\n",
    "\n",
    "    # Get its state dict\n",
    "    state_dict = temp.state_dict()\n",
    "\n",
    "    # Change the last layer to fit our, smaller network\n",
    "    state_dict['classifier.6.weight'] = torch.randn(10, 4096)\n",
    "    state_dict['classifier.6.bias'] = torch.randn(10)\n",
    "\n",
    "    # Apply the state dict and set the classifer (layer part) to be the only thing we train\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.optim = torch.optim.Adam(model.classifier.parameters())\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(x: list):\n",
    "    # model = VGG16()\n",
    "    # train(model)\n",
    "    # model.accuracy\n",
    "    \n",
    "    return # - model.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = (1e-3, 1e-5)\n",
    "final_layer = (3000, 6000)\n",
    "x0 = [np.random.uniform(learning_rate[0], learning_rate[1]), np.random.randint(final_layer[0], final_layer[1])]\n",
    "y0 = objective_function(x0)\n",
    "\n",
    "opt = gp_minimize(objective_function,\n",
    "            [learning_rate, final_layer],\n",
    "            acq_func= \"EI\",\n",
    "            n_initial_points= 0,\n",
    "            n_calls= 19,\n",
    "            x0= [x0,],\n",
    "            xi= 0.1\n",
    "            # noise=\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
