{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import PIL\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import pickle\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "\n",
    "#ACTUALLY needed for OUR PROJECT\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device:',device)\n",
    "\n",
    "# Set a random seed for everything important\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a seed with a random integer, in this case, I choose my verymost favourite sequence of numbers\n",
    "seed_everything(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3220 examples in the training set\n",
      "There are 690 examples in the test set \n",
      "\n",
      "Image shape is: torch.Size([128, 128]), label example is 0\n"
     ]
    }
   ],
   "source": [
    "class TumorDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        target = torch.tensor(self.data[idx][0], dtype=torch.long)  # Ensure target is long for classification\n",
    "        image = torch.tensor(self.data[idx][1], dtype=torch.float32)  # Convert image to float32\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Use len() instead of shape[0] for lists\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(x_.to(device) for x_ in default_collate(batch))\n",
    "\n",
    "def get_dataset(test_size, val_size, v=True): \n",
    "    with open(\"dataset.pkl\", \"rb\") as f: \n",
    "        rawdata = pickle.load(f)\n",
    "\n",
    "    labels = {\n",
    "        \"Not cancer\": 0, \n",
    "        \"Cancer\": 1\n",
    "    }\n",
    "\n",
    "    dataset = TumorDataset(data=rawdata)\n",
    "\n",
    "    test_amount, val_amount = int(dataset.__len__() * test_size), int(dataset.__len__() * val_size)\n",
    "\n",
    "    # this function will automatically randomly split your dataset but you could also implement the split yourself\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(dataset, [\n",
    "                (dataset.__len__() - (test_amount + val_amount)), \n",
    "                test_amount, \n",
    "                val_amount\n",
    "    ])\n",
    "    \n",
    "    print(f\"There are {len(train_set)} examples in the training set\")\n",
    "    print(f\"There are {len(test_set)} examples in the test set \\n\")\n",
    "    print(f\"Image shape is: {train_set[0][0].shape}, label example is {train_set[0][1]}\")\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "train_set, val_set, test_set = get_dataset(test_size=0.15, val_size=0.15)\n",
    "\n",
    "# Make dataloaders\n",
    "batch_size=16\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16D(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, features_fore_linear=25088, dataset=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Helper hyperparameters to keep track of VGG16 architecture\n",
    "        conv_stride = ...\n",
    "        pool_stride = ...\n",
    "        conv_kernel = ...\n",
    "        pool_kernel = ...\n",
    "        dropout_probs = ...\n",
    "        optim_momentum = ...\n",
    "        weight_decay = ...\n",
    "        learning_rate = ...\n",
    "\n",
    "        # Define features and classifier each individually, this is how the VGG16-D model is orignally defined\n",
    "        # Define the VGG16-D convolutional (feature extraction) layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Define the VGG16-D fully connected (classification) layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(8192, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "        # In the paper, they mention updating towards the 'multinomial logistic regression objective'\n",
    "        # As can be read in Bishop p. 159, taking the logarithm of this equates to the cross-entropy loss function.\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer - For now just set to Adam to test the implementation\n",
    "        self.optim = torch.optim.Adam(list(self.features.parameters()) + list(self.classifier.parameters()), lr=0.001)\n",
    "        # self.optim = torch.optim.SGD(list(self.features.parameters()) + list(self.classifier.parameters()), lr=learning_rate, momentum=optim_momentum, weight_decay=weight_decay)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def train_model(self, train_dataloader, epochs=1, val_dataloader=None):\n",
    "        \n",
    "        # Call .train() on self to turn on dropout\n",
    "        self.train()\n",
    "\n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "        \n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # Keep track of training accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "\n",
    "            # If val_dataloader, evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                # Turn off dropout for testing\n",
    "                self.eval()\n",
    "                acc = self.eval_model(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}, test accuracy: {epoch_acc / len(train_dataloader.dataset)}\")\n",
    "                # turn on dropout after being done\n",
    "                self.train()\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval_model(self, test_dataloader):\n",
    "        \n",
    "        self.eval()\n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            total_acc += (torch.argmax(logits, dim=1) == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "\n",
    "\n",
    "    def predict(self, img_path):\n",
    "        img = PIL.Image.open(img_path)\n",
    "        img = self.dataset.dataset.transform(img)\n",
    "        classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "        return img, classification\n",
    "    \n",
    "    def predict_random(self, num_predictions=16):\n",
    "        \"\"\"\n",
    "        Plot random images from own given dataset\n",
    "        \"\"\"\n",
    "        random_indices = np.random.choice(len(self.dataset)-1, num_predictions, replace=False)\n",
    "        classifcations = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        for idx in random_indices:\n",
    "            img, label = self.dataset.__getitem__(idx)\n",
    "\n",
    "            classifcation = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "\n",
    "            classifcations.append(classifcation)\n",
    "            labels.append(label)\n",
    "            images.append(img)\n",
    "\n",
    "        return classifcations, labels, images\n",
    "\n",
    "def get_vgg_weights(model):\n",
    "    \"\"\"\n",
    "    Loads VGG16-D weights for the classifier to an already existing model\n",
    "    Also sets training to only the classifier\n",
    "    \"\"\"\n",
    "    # Load the complete VGG16 model\n",
    "    temp = torchvision.models.vgg16(weights='DEFAULT')\n",
    "\n",
    "    # Get its state dict\n",
    "    state_dict = temp.state_dict()\n",
    "\n",
    "    # Change the last layer to fit our, smaller network\n",
    "    state_dict['classifier.6.weight'] = torch.randn(10, 4096)\n",
    "    state_dict['classifier.6.bias'] = torch.randn(10)\n",
    "\n",
    "    # Apply the state dict and set the classifer (layer part) to be the only thing we train\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.optim = torch.optim.Adam(model.classifier.parameters())\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing weights from vgg16d_weights.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/m42979wx2659hjrdgtlcvzbm0000gn/T/ipykernel_12483/3024856344.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  CNN_model.load_state_dict(torch.load(weights_path))\n",
      "  6%|â–‹         | 13/202 [00:03<00:55,  3.41it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m train_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 30\u001b[0m train_accs, test_accs \u001b[38;5;241m=\u001b[39m \u001b[43mCNN_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# **Save model weights after training**\u001b[39;00m\n\u001b[1;32m     33\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(CNN_model\u001b[38;5;241m.\u001b[39mstate_dict(), weights_path)\n",
      "Cell \u001b[0;32mIn[46], line 108\u001b[0m, in \u001b[0;36mVGG16D.train_model\u001b[0;34m(self, train_dataloader, epochs, val_dataloader)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Keep track of training accuracy\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     epoch_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m train_accs\u001b[38;5;241m.\u001b[39mappend(epoch_acc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader\u001b[38;5;241m.\u001b[39mdataset))\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# If val_dataloader, evaluate after each epoch\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the in_channels and input dimensions from the first batch\n",
    "in_channels = 1#next(iter(train_dataloader))[0].shape[1]\n",
    "in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "\n",
    "# Create a dummy model to find the dimension before the first linear layer\n",
    "CNN_model = VGG16D(num_classes=2, in_channels=in_channels)\n",
    "CNN_model.to(device)\n",
    "\n",
    "dummy_input = torch.randn(1, in_channels, in_width_height, in_width_height).to(device)\n",
    "dummy_output = CNN_model.features(dummy_input)\n",
    "n_features = dummy_output.shape[1]\n",
    "\n",
    "# Path to save and load model weights\n",
    "weights_path = 'vgg16d_weights.pth'\n",
    "\n",
    "# Initialize the model\n",
    "CNN_model = VGG16D(num_classes=2, in_channels=in_channels, features_fore_linear=n_features, dataset=test_set)\n",
    "CNN_model.to(device)  # Move model to MPS, CUDA, or CPU\n",
    "\n",
    "# **Load model weights if available**\n",
    "# if os.path.isfile(weights_path):\n",
    "#     print(f\"Loading existing weights from {weights_path}...\")\n",
    "#     CNN_model.load_state_dict(torch.load(weights_path))\n",
    "# else:\n",
    "#     print(\"No pre-trained weights found. Initializing new model...\")\n",
    "\n",
    "\n",
    "# Train model\n",
    "train_epochs = 10\n",
    "train_accs, test_accs = CNN_model.train_model(train_dataloader, epochs=train_epochs, val_dataloader=test_dataloader)\n",
    "\n",
    "# **Save model weights after training**\n",
    "torch.save(CNN_model.state_dict(), weights_path)\n",
    "print(f\"Model weights saved to {weights_path}.\")\n",
    "\n",
    "# Check if previous accuracy files exist, and load them\n",
    "if os.path.isfile('train_accs.pkl'):\n",
    "    with open('train_accs.pkl', 'rb') as f:\n",
    "        old_train_accs = pickle.load(f)\n",
    "else:\n",
    "    old_train_accs = []\n",
    "\n",
    "if os.path.isfile('test_accs.pkl'):\n",
    "    with open('test_accs.pkl', 'rb') as f:\n",
    "        old_test_accs = pickle.load(f)\n",
    "else:\n",
    "    old_test_accs = []\n",
    "\n",
    "# Append new accuracies to the existing list\n",
    "all_train_accs = old_train_accs + train_accs\n",
    "all_test_accs = old_test_accs + test_accs\n",
    "\n",
    "# Save back to the same files\n",
    "with open('train_accs.pkl', 'wb') as f:\n",
    "    pickle.dump(all_train_accs, f)\n",
    "\n",
    "with open('test_accs.pkl', 'wb') as f:\n",
    "    pickle.dump(all_test_accs, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
