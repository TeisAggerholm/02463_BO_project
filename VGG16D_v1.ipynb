{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import PIL\n",
    "from torch import nn\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import pickle\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "\n",
    "#ACTUALLY needed for OUR PROJECT\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device:',device)\n",
    "\n",
    "# Set a random seed for everything important\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set a seed with a random integer, in this case, I choose my verymost favourite sequence of numbers\n",
    "seed_everything(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3220 examples in the training set\n",
      "There are 690 examples in the test set \n",
      "\n",
      "Image shape is: torch.Size([128, 128]), label example is 0\n"
     ]
    }
   ],
   "source": [
    "class TumorDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        target = torch.tensor(self.data[idx][0])\n",
    "        image = torch.tensor(self.data[idx][1])\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.data).shape[0]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(x_.to(device) for x_ in default_collate(batch))\n",
    "\n",
    "def get_dataset(test_size, val_size, v=True): \n",
    "    with open(\"dataset.pkl\", \"rb\") as f: \n",
    "        rawdata = pickle.load(f)\n",
    "\n",
    "    dataset = TumorDataset(data=rawdata)\n",
    "\n",
    "    test_amount, val_amount = int(dataset.__len__() * test_size), int(dataset.__len__() * val_size)\n",
    "\n",
    "    # this function will automatically randomly split your dataset but you could also implement the split yourself\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(dataset, [\n",
    "                (dataset.__len__() - (test_amount + val_amount)), \n",
    "                test_amount, \n",
    "                val_amount\n",
    "    ])\n",
    "    \n",
    "    print(f\"There are {len(train_set)} examples in the training set\")\n",
    "    print(f\"There are {len(test_set)} examples in the test set \\n\")\n",
    "    print(f\"Image shape is: {train_set[0][0].shape}, label example is {train_set[0][1]}\")\n",
    "    \n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "train_set, val_set, test_set = get_dataset(test_size=0.15, val_size=0.15)\n",
    "\n",
    "# Make dataloaders\n",
    "batch_size=16\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_dataloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16D(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, features_fore_linear=25088, dataset=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Helper hyperparameters to keep track of VGG16 architecture\n",
    "        conv_stride = ...\n",
    "        pool_stride = ...\n",
    "        conv_kernel = ...\n",
    "        pool_kernel = ...\n",
    "        dropout_probs = ...\n",
    "        optim_momentum = ...\n",
    "        weight_decay = ...\n",
    "        learning_rate = ...\n",
    "\n",
    "        # Define features and classifier each individually, this is how the VGG16-D model is orignally defined\n",
    "        # Define the VGG16-D convolutional (feature extraction) layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Define the VGG16-D fully connected (classification) layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "        # In the paper, they mention updating towards the 'multinomial logistic regression objective'\n",
    "        # As can be read in Bishop p. 159, taking the logarithm of this equates to the cross-entropy loss function.\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Optimizer - For now just set to Adam to test the implementation\n",
    "        self.optim = torch.optim.Adam(list(self.features.parameters()) + list(self.classifier.parameters()), lr=0.001)\n",
    "        # self.optim = torch.optim.SGD(list(self.features.parameters()) + list(self.classifier.parameters()), lr=learning_rate, momentum=optim_momentum, weight_decay=weight_decay)\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def train_model(self, train_dataloader, epochs=1, val_dataloader=None):\n",
    "        \n",
    "        # Call .train() on self to turn on dropout\n",
    "        self.train()\n",
    "\n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # Keep track of training accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "\n",
    "            # If val_dataloader, evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                # Turn off dropout for testing\n",
    "                self.eval()\n",
    "                acc = self.eval_model(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}\")\n",
    "                # turn on dropout after being done\n",
    "                self.train()\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval_model(self, test_dataloader):\n",
    "        \n",
    "        self.eval()\n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            total_acc += (torch.argmax(logits, dim=1) == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "\n",
    "    def predict(self, img_path):\n",
    "        img = PIL.Image.open(img_path)\n",
    "        img = self.dataset.dataset.transform(img)\n",
    "        classification = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "        return img, classification\n",
    "    \n",
    "    def predict_random(self, num_predictions=16):\n",
    "        \"\"\"\n",
    "        Plot random images from own given dataset\n",
    "        \"\"\"\n",
    "        random_indices = np.random.choice(len(self.dataset)-1, num_predictions, replace=False)\n",
    "        classifcations = []\n",
    "        labels = []\n",
    "        images = []\n",
    "        for idx in random_indices:\n",
    "            img, label = self.dataset.__getitem__(idx)\n",
    "\n",
    "            classifcation = torch.argmax(self(img.unsqueeze(dim=0)), dim=1)\n",
    "\n",
    "            classifcations.append(classifcation)\n",
    "            labels.append(label)\n",
    "            images.append(img)\n",
    "\n",
    "        return classifcations, labels, images\n",
    "\n",
    "def get_vgg_weights(model):\n",
    "    \"\"\"\n",
    "    Loads VGG16-D weights for the classifier to an already existing model\n",
    "    Also sets training to only the classifier\n",
    "    \"\"\"\n",
    "    # Load the complete VGG16 model\n",
    "    temp = torchvision.models.vgg16(weights='DEFAULT')\n",
    "\n",
    "    # Get its state dict\n",
    "    state_dict = temp.state_dict()\n",
    "\n",
    "    # Change the last layer to fit our, smaller network\n",
    "    state_dict['classifier.6.weight'] = torch.randn(10, 4096)\n",
    "    state_dict['classifier.6.bias'] = torch.randn(10)\n",
    "\n",
    "    # Apply the state dict and set the classifer (layer part) to be the only thing we train\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model.optim = torch.optim.Adam(model.classifier.parameters())\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-trained weights found. Initializing new model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/592 [00:11<11:15,  1.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     31\u001b[0m train_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 32\u001b[0m train_accs, test_accs \u001b[38;5;241m=\u001b[39m \u001b[43mCNN_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# **Save model weights after training**\u001b[39;00m\n\u001b[1;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(CNN_model\u001b[38;5;241m.\u001b[39mstate_dict(), weights_path)\n",
      "Cell \u001b[0;32mIn[36], line 100\u001b[0m, in \u001b[0;36mVGG16D.train_model\u001b[0;34m(self, train_dataloader, epochs, val_dataloader)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     98\u001b[0m     epoch_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/imagenette.py:93\u001b[0m, in \u001b[0;36mImagenette.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, Any]:\n\u001b[1;32m     92\u001b[0m     path, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_samples[idx]\n\u001b[0;32m---> 93\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/PIL/Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get the in_channels and input dimensions from the first batch\n",
    "in_channels = next(iter(train_dataloader))[0].shape[1]\n",
    "in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "\n",
    "# Create a dummy model to find the dimension before the first linear layer\n",
    "CNN_model = VGG16D(num_classes=10, in_channels=in_channels)\n",
    "\n",
    "# WARNING - THIS PART MIGHT BREAK\n",
    "features_fore_linear = utils.get_dim_before_first_linear(CNN_model.features, in_width_height, in_channels, brain=True)\n",
    "\n",
    "dummy_input = torch.randn(1, in_channels, in_width_height, in_width_height)\n",
    "dummy_output = CNN_model.features(dummy_input)\n",
    "n_features = dummy_output.shape[1]\n",
    "\n",
    "# Path to save and load model weights\n",
    "weights_path = 'vgg16d_weights.pth'\n",
    "\n",
    "# Initialize the model\n",
    "CNN_model = VGG16D(num_classes=10, in_channels=in_channels, features_fore_linear=n_features, dataset=test_set)\n",
    "CNN_model.to(device)  # Move model to MPS, CUDA, or CPU\n",
    "\n",
    "# **Load model weights if available**\n",
    "if os.path.isfile(weights_path):\n",
    "    print(f\"Loading existing weights from {weights_path}...\")\n",
    "    CNN_model.load_state_dict(torch.load(weights_path))\n",
    "else:\n",
    "    print(\"No pre-trained weights found. Initializing new model...\")\n",
    "\n",
    "\n",
    "# Train model\n",
    "train_epochs = 2\n",
    "train_accs, test_accs = CNN_model.train_model(train_dataloader, epochs=train_epochs, val_dataloader=test_dataloader)\n",
    "\n",
    "# **Save model weights after training**\n",
    "torch.save(CNN_model.state_dict(), weights_path)\n",
    "print(f\"Model weights saved to {weights_path}.\")\n",
    "\n",
    "# Check if previous accuracy files exist, and load them\n",
    "if os.path.isfile('train_accs.pkl'):\n",
    "    with open('train_accs.pkl', 'rb') as f:\n",
    "        old_train_accs = pickle.load(f)\n",
    "else:\n",
    "    old_train_accs = []\n",
    "\n",
    "if os.path.isfile('test_accs.pkl'):\n",
    "    with open('test_accs.pkl', 'rb') as f:\n",
    "        old_test_accs = pickle.load(f)\n",
    "else:\n",
    "    old_test_accs = []\n",
    "\n",
    "# Append new accuracies to the existing list\n",
    "all_train_accs = old_train_accs + train_accs\n",
    "all_test_accs = old_test_accs + test_accs\n",
    "\n",
    "# Save back to the same files\n",
    "with open('train_accs.pkl', 'wb') as f:\n",
    "    pickle.dump(all_train_accs, f)\n",
    "\n",
    "with open('test_accs.pkl', 'wb') as f:\n",
    "    pickle.dump(all_test_accs, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
